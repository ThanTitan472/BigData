{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.appName('heart_detection').config(\"spark.python.worker.reuse\", \"true\").config(\"spark.python.worker.timeout\", \"600\").getOrCreate()\n",
    "df = spark.read.csv('heartrate_seconds_merged_3.12.16-4.11.16.csv', header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"datetime_column\", to_timestamp(df[\"Time\"], \"MM/dd/yyyy hh:mm:ss a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(\n",
    "    'Id',\n",
    "    'datetime_column',\n",
    "    'Value'\n",
    ")\n",
    "df = df.withColumnRenamed(\"Value\", \"Heartrate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Tính toán sự thay đổi giữa các giá trị nhịp tim liên tiếp\n",
    "windowSpec = Window.partitionBy(\"Id\").orderBy(\"datetime_column\")\n",
    "df = df.withColumn(\"prev_heartrate\", F.lag(\"Heartrate\", 1).over(windowSpec))\n",
    "df = df.withColumn(\"heartrate_change\", F.col(\"Heartrate\") - F.col(\"prev_heartrate\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    \"label\", \n",
    "    F.when(\n",
    "        F.abs(F.col(\"heartrate_change\")) > 10, 0.5\n",
    "    ).when(\n",
    "        (F.col(\"Heartrate\") > 100) | (F.col(\"Heartrate\") < 60), 0\n",
    "    ).otherwise(1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+---------+--------------+----------------+-----+\n",
      "|        Id|    datetime_column|Heartrate|prev_heartrate|heartrate_change|label|\n",
      "+----------+-------------------+---------+--------------+----------------+-----+\n",
      "|2026352035|2016-04-02 00:08:20|       62|          NULL|            NULL|  1.0|\n",
      "|2026352035|2016-04-02 00:08:35|       62|            62|               0|  1.0|\n",
      "|2026352035|2016-04-02 00:08:50|       61|            62|              -1|  1.0|\n",
      "|2026352035|2016-04-02 00:09:00|       62|            61|               1|  1.0|\n",
      "|2026352035|2016-04-02 00:09:15|       62|            62|               0|  1.0|\n",
      "+----------+-------------------+---------+--------------+----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, collect_list, avg, when, lit\n",
    "\n",
    "# Chuyển đổi cột 'datetime_column' thành kiểu timestamp thay vì chỉ date\n",
    "grouped_df = df.withColumn('timestamp', to_timestamp(col('datetime_column'))) \\\n",
    "    .withColumn('date', col('timestamp').cast('date'))  # Tạo cột 'date' chỉ chứa ngày\n",
    "\n",
    "# Gom nhóm theo ngày\n",
    "grouped_df = grouped_df.groupBy('Id','date').agg(\n",
    "    collect_list(\"Heartrate\").alias(\"heartrate_list\"),\n",
    "    collect_list(\"prev_heartrate\").alias(\"prev_heartrate_list\"),\n",
    "    collect_list(\"heartrate_change\").alias(\"heartrate_change_list\"),\n",
    "    avg(\"label\").alias(\"mean_label\")  # Tính trung bình trên cột label\n",
    ")\n",
    "\n",
    "# Tạo cột 'final_label' dựa trên giá trị trung bình của 'label'\n",
    "grouped_df = grouped_df.withColumn(\n",
    "    \"final_label\", \n",
    "    when(col(\"mean_label\") > 0.8, lit(2))\n",
    "        .when(col(\"mean_label\") < 0.5, lit(0))\n",
    "        .otherwise(lit(1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+-----------+\n",
      "|        Id|      date|      heartrate_list|final_label|\n",
      "+----------+----------+--------------------+-----------+\n",
      "|2347167796|2016-03-29|[69, 68, 69, 69, ...|          2|\n",
      "|6962181067|2016-03-30|[60, 60, 59, 59, ...|          2|\n",
      "|2347167796|2016-03-30|[58, 59, 60, 60, ...|          1|\n",
      "|6962181067|2016-03-31|[63, 62, 62, 63, ...|          1|\n",
      "|2347167796|2016-03-31|[81, 79, 80, 79, ...|          2|\n",
      "|4020332650|2016-04-01|[71, 71, 71, 71, ...|          1|\n",
      "|5553957443|2016-04-01|[68, 66, 67, 66, ...|          2|\n",
      "|6962181067|2016-04-01|[66, 67, 66, 65, ...|          2|\n",
      "|8792009665|2016-04-01|[81, 81, 81, 81, ...|          2|\n",
      "|6775888955|2016-04-01|[121, 120, 122, 1...|          0|\n",
      "|2022484408|2016-04-01|[93, 91, 96, 98, ...|          1|\n",
      "|7007744171|2016-04-01|[87, 87, 87, 91, ...|          2|\n",
      "|2347167796|2016-04-01|[65, 65, 62, 65, ...|          2|\n",
      "|4558609924|2016-04-01|[67, 70, 70, 68, ...|          2|\n",
      "|5577150313|2016-04-01|[55, 55, 55, 57, ...|          0|\n",
      "|6117666160|2016-04-01|[74, 74, 73, 74, ...|          2|\n",
      "|8877689391|2016-04-01|[74, 70, 71, 79, ...|          1|\n",
      "|4020332650|2016-04-02|[62, 62, 62, 62, ...|          1|\n",
      "|2026352035|2016-04-02|[62, 62, 61, 62, ...|          2|\n",
      "|6962181067|2016-04-02|[70, 73, 72, 74, ...|          1|\n",
      "+----------+----------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bước 2: Hiển thị kết quả\n",
    "grouped_df.select(\"Id\",\"date\", \"heartrate_list\", \"final_label\").orderBy('date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+-----------+\n",
      "|          overchange|            overmax|             overmin|final_label|\n",
      "+--------------------+-------------------+--------------------+-----------+\n",
      "|                 0.0|                0.0| 0.04100227790432802|          2|\n",
      "| 0.00195031820981318|0.12460227855896541|0.014574566355332033|          2|\n",
      "|0.002457002457002457|                0.0| 0.19926289926289925|          1|\n",
      "|0.002334267040149393|                0.0| 0.42250233426704015|          1|\n",
      "|0.001195652173913...| 0.0932608695652174| 0.19956521739130434|          1|\n",
      "+--------------------+-------------------+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, size, expr\n",
    "\n",
    "# Tính overmin: Phần trăm giá trị nhỏ hơn 60\n",
    "grouped_df = grouped_df.withColumn(\n",
    "    \"overmin\",\n",
    "    size(expr(\"filter(heartrate_list, x -> x < 60)\")) / size(col(\"heartrate_list\"))\n",
    ")\n",
    "\n",
    "# Tính overmax: Phần trăm giá trị lớn hơn 100\n",
    "grouped_df = grouped_df.withColumn(\n",
    "    \"overmax\",\n",
    "    size(expr(\"filter(heartrate_list, x -> x > 100)\")) / size(col(\"heartrate_list\"))\n",
    ")\n",
    "\n",
    "# Tính overchange: Phần trăm giá trị lớn hơn 10 hoặc nhỏ hơn -10\n",
    "grouped_df = grouped_df.withColumn(\n",
    "    \"overchange\",\n",
    "    size(expr(\"filter(heartrate_change_list, x -> x > 10 OR x < -10)\")) / size(col(\"heartrate_change_list\"))\n",
    ")\n",
    "\n",
    "grouped_df = grouped_df.select(\"overchange\", \"overmax\", \"overmin\", \"final_label\")\n",
    "grouped_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"overchange\", \"overmax\", \"overmin\"], outputCol=\"features\")\n",
    "grouped_df = assembler.transform(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Khởi tạo mô hình RandomForest\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"final_label\", numTrees=50)\n",
    "# lr = LogisticRegression(featuresCol=\"features\", labelCol=\"final_label\")\n",
    "# Huấn luyện mô hình\n",
    "heartrate_model = rf.fit(grouped_df)\n",
    "# model = lr.fit(grouped_df_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.986013986013986\n"
     ]
    }
   ],
   "source": [
    "# 4. Dự đoán trên dữ liệu kiểm tra (giả sử test_data có cùng cấu trúc với grouped_df)\n",
    "predictions = heartrate_model.transform(grouped_df)\n",
    "\n",
    "# 5. Đánh giá mô hình\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"final_label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, to_timestamp, collect_list, avg, when, lit\n",
    "from pyspark.sql.functions import col, size, expr\n",
    "\n",
    "def preprocess_data(df):\n",
    "  df = df.withColumn(\"datetime_column\", to_timestamp(df[\"Time\"], \"MM/dd/yyyy hh:mm:ss a\"))\n",
    "  df = df.select(\n",
    "      'Id',\n",
    "      'datetime_column',\n",
    "      'Value'\n",
    "  )\n",
    "  df = df.withColumnRenamed(\"Value\", \"Heartrate\")\n",
    "\n",
    "  windowSpec = Window.partitionBy(\"Id\").orderBy(\"datetime_column\")\n",
    "  df = df.withColumn(\"prev_heartrate\", F.lag(\"Heartrate\", 1).over(windowSpec))\n",
    "  df = df.withColumn(\"heartrate_change\", F.col(\"Heartrate\") - F.col(\"prev_heartrate\"))\n",
    "\n",
    "  df = df.withColumn('timestamp', to_timestamp(col('datetime_column'))).withColumn('date', col('timestamp').cast('date'))\n",
    "\n",
    "  df = df.groupBy('Id','date').agg(\n",
    "      collect_list(\"Heartrate\").alias(\"heartrate_list\"),\n",
    "      collect_list(\"prev_heartrate\").alias(\"prev_heartrate_list\"),\n",
    "      collect_list(\"heartrate_change\").alias(\"heartrate_change_list\")\n",
    "  )\n",
    "\n",
    "  df = df.withColumn(\n",
    "      \"overmin\",\n",
    "      size(expr(\"filter(heartrate_list, x -> x < 60)\")) / size(col(\"heartrate_list\"))\n",
    "  )\n",
    "\n",
    "  df = df.withColumn(\n",
    "      \"overmax\",\n",
    "      size(expr(\"filter(heartrate_list, x -> x > 100)\")) / size(col(\"heartrate_list\"))\n",
    "  )\n",
    "\n",
    "  df = df.withColumn(\n",
    "      \"overchange\",\n",
    "      size(expr(\"filter(heartrate_change_list, x -> x > 10 OR x < -10)\")) / size(col(\"heartrate_change_list\"))\n",
    "  )\n",
    "\n",
    "  df = df.select(\"overchange\", \"overmax\", \"overmin\")\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(df):\n",
    "  assembler = VectorAssembler(inputCols=[\"overchange\", \"overmax\", \"overmin\"], outputCol=\"features\")\n",
    "  df = assembler.transform(df)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from pymongo import MongoClient\n",
    "from json import loads\n",
    "\n",
    "\n",
    "kafka_topic = 'BigData_Heartrate_Predict'\n",
    "kafka_server = 'localhost:9092'\n",
    "mongo_uri = 'mongodb://localhost:27017'\n",
    "mongo_db = 'BigData_Heartrate'\n",
    "mongo_collection = 'Predict'\n",
    "\n",
    "mongo_client = MongoClient(mongo_uri)\n",
    "mongo_db = mongo_client[mongo_db]\n",
    "mongo_collection = mongo_db[mongo_collection]\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    kafka_topic,\n",
    "    bootstrap_servers=kafka_server,\n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=True,\n",
    "    group_id='do_an',\n",
    "    value_deserializer=lambda x: loads(x.decode('utf-8'))\n",
    ")\n",
    "data_list = []\n",
    "try:\n",
    "    for message in consumer:\n",
    "        data = message.value\n",
    "        data_list.append(data)\n",
    "        print(f\"Recived: {data}\")\n",
    "        df = spark.createDataFrame(data_list)\n",
    "        df = preprocess_data(df)\n",
    "        df = vectorize_data(df)\n",
    "        df_predict = heartrate_model.transform(df)\n",
    "        row_count = df_predict.count()\n",
    "        if row_count > 1:\n",
    "            data_list.clean()\n",
    "            data_list.append(data)\n",
    "        predict = df_predict.tail(1)[0]['prediction']\n",
    "        predict_str = \"\"\n",
    "        if predict == 0:\n",
    "            predict_str = \"Nhịp tim quá cao/quá thấp\"\n",
    "        elif predict == 1:\n",
    "            predict_str = \"Nhịp tim không ổn định\"\n",
    "        else:\n",
    "            predict_str = \"Nhịp tim bình thường\"\n",
    "        data[\"Heartrate\"] = data.pop(\"Value\")\n",
    "        data[\"Prediction\"] = predict_str\n",
    "        print(f\"Result row: {data}\")\n",
    "        mongo_collection.insert_one(data)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nStopped receive data.\")\n",
    "consumer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
